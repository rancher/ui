---
languageName: "简体中文"
languageContribute: "帮助翻译Rancher"

loggingPage:
  targetNav:
    experimental: 实验性
    tips:
      caseA: 当前{pageScope}日志收集未启用
      caseB: 当前日志收集目标是<code class="text-capitalize">{currentTarget}</code>,点击下面的保存按钮将禁止当前{pageScope}的日志收集
      caseC: 当前日志收集目标是<code class="text-capitalize">{currentTarget}</code>
      caseD: 点击下面的保存按钮去设置<span class="text-info text-capitalize">{targetType}</span>为日志收集目标
      caseE: 点击下面的保存按钮来更新<span class="text-info text-capitalize">{targetType}</span>配置
      caseF: 没有日志收集目标服务, 点击下面的保存按钮去设置<code class="text-capitalize">{targetType}</code>为日志收集目标服务
  targetTypes:
    embedded: Embedded
    elasticsearch: Elasticsearch
    splunk: Splunk
    kafka: Kafka
    syslog: syslog
    disable: None
  endpoint: 访问地址
  endpointPlaceholder: '例如：https://192.168.1.10:9200'
  logging: 日志收集
  clusterHeader: 集群日志收集
  projectHeader: 项目日志收集
  helpText:
    cluster: 我们将为每个容器收集标准输出和标准错误，<code>/var/log/containers/</code>是每个主机日志文件路径，日志将被发送到您在下面选择的目标服务
    clusterTarget: 当前日志收集目标是<code class="text-capitalize">{clusterTargetType}</code>. 如果项目日志收集启用，日志将被发送到集群目标和项目目标
    noClusterTarget: 当前集群日志收集已禁用
  tags:
    keyPlaceholder: 例如：foo
    valuePlaceholder: 例如：bar
    addActionLabel: 添加字段
  keyValueForm:
    keyPlaceholder: 例如：192.168.1.10
  targetKafka:
    addActionLabel: 添加代理
    host: 主机
    port: 端口
  embedded:
    elasticsearchEndpoint: 'Elascticsearch Endpoint:'
    kibanaEndpoint: 'Kibana Endpoint:'
    header: 内置Elasticsearch配置
    failed: 失败
    pending: 部署
    helpText:
      persistantStorage: 内置Elasticsearch是实验性的, 不会提供持久性存储
      requirements: 内置Elasticsearch包括Elasticsearch和Kibana，Elasticsearch在部署的节点上至少需要1个CPU和500M内存
    cpuRequests:
      label: CPU需求(Core)
      placeholder: 例如：1
      helpText: 至少需要1个内核CPU
    cpuLimits:
      label: CPU限制(Core)
      placeholder: 例如：1
      helpText: 至少需要1个内核CPU
    memRequests:
      label: 内存需求(M)
      placeholder: 例如：512M
      helpText: 至少需要512M RAM
    memLimits:
      label: 内存限制(M)
      placeholder: 例如：2000
      helpText: 至少需要512M RAM
    resouceRequestsAndLimits: CPU和Memory

    indexPatterns:
      header: 索引模式
      helpText: 索引模式用于生成Elacticsearch索引
      prefix: 前缀
      prefixPlaceholder: 例如：logstash
      dateFormat: '日期格式:'
    generatedIndex: '生成的索引格式为: <code class="text-italic">{esIndex}</code>, 默认情况下，索引模式为 $\{clusterName\}-$\{dateFormat\}'
  logPreview:
    header: 日志格式预览
  additional:
    header: 其他日志配置
    fields:
      header: 自定义Log字段
      helpText: 你可以以键值对的形式添加自定义字段，以便更好地进行过滤
    flushInterval:
      header: 刷新时间间隔
      label: 刷新时间间隔
      placeholder: 例如：1
      sec: 秒
      helpText: 日志刷新频率.

  elasticsearch:
    header: Elasticsearch配置
    endpointHelpText: 输入云端Elacticsearch地址，或者私有部署的Elacticsearch地址
    xpack:
      header: X-Pack安全
      headerOptional: (可选)
      helpText: 如果您的Elasticsearch开启了X-Pack内置的本地身份验证功能，请在下面设置用户名和密码
      username: 用户名
      usernamePlaceholder: 例如：John
      password: 密码
      passwordPlaceholder: 你的密码
    indexPatterns:
      header: 索引模式
      helpText: 索引模式用于生成Elacticsearch索引
      prefix: 前缀
      prefixPlaceholder: 例如：logstash
      dateFormat: '日期格式:'
    generatedIndex: '生成的索引格式为: <code class="text-italic">{esIndex}</code>, 默认情况下，索引模式为：{indexFormat}'
  splunk:
    header: Splunk HTTP事件收集配置
    helpText: '您可以找到如何配置Splunk HEC(HTTP事件收集器)的说明,<a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector" target="_blank">点击这里</a>.'
    token: Token
    tokenPlaceholder: Your Token
    tokenHelpText: 'Token是允许日志收集程序和HTTP客户端连接到HEC的验证信息,<a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Enterprise" target="_blank">了解详情</a>'
    source: 日志源
    sourcePlaceholder: 例如：fluentd
    sourceHelpText: '标识事件来源的默认字段，即事件发生的位置，<a href="https://docs.splunk.com/Splexicon:Source" target="_blank">了解详情</a>'
  kafka:
    header: Kafka配置
    endpointType: 访问端点类型
    zookeeper: Zookeeper
    broker: Broker
    brokerTypeHelpText: 使用Zookeeper或Broker作为Kafka连接入口点
    zookeeperHelpText: Zookeeper用于构建协调，配置管理，领导者检测，检测kafka集群中的节点更新
    brokkerHelpText: Kafka集群由一个或多个Broker组成，为每个Broker配置主机和端口
    addEndpoint: 访问地址
    topic: 主题
    topicPlaceholder: 例如：message
    topicHelpText: 日志将发送到这个主题
  syslog:
    endpointPlaceholder: '例如：192.168.1.10:514'
    header: Syslog配置
    endpointHelpText: 在这里输入日志服务器的接入地址
    program: 程序名称
    programPlaceholder: 例如：MyProgram
    programHelpText: 日志中的程序名称
    severityHelpText: '<p class="text-info text-small">日志的严重性列表可以在这里找到，<a href="https://tools.ietf.org/html/rfc5424#section-6.2.1" target="_blank">了解详情</a></p>'
    severities:
      emergency: Emergency
      alert: alert
      critical: Ctitical
      error: error
      warning: warning
      notice: notice
      info: Info
      debug: Debug
