loggingPage:
  targetNav:
    experimental: Experimental
    tips:
      caseA: Logging is disabled in the current {pageScope}.
      caseB: The current logging target is <code class="text-capitalize">{currentTarget}</code>, Click the Save button below will disable logging in current {pageScope}.
      caseC: The current logging target is <code class="text-capitalize">{currentTarget}</code>,
      caseD: click the Save button below to set <span class="text-info text-capitalize">{targetType}</span> as the logging target.
      caseE: click the Save button below to update the <span class="text-info text-capitalize">{targetType}</span> configuration.
      caseF: No logging target, click the Save button below to set <code class="text-capitalize">{targetType}</code> as the logging target.
  targetTypes:
    embedded: Embedded
    elasticsearch: Elasticsearch
    splunk: Splunk
    kafka: Kafka
    syslog: Syslog
    fluentd: Fluentd
    cloudwatch: CloudWatch
    disable: None
  endpoint: Endpoint
  endpointPlaceholder: 'e.g. https://192.168.1.10:9200'
  logging: Logging
  clusterHeader: Cluster Logging
  projectHeader: Project Logging
  helpText:
    cluster: We will use fluentd to collect stdout/stderr logs from each container and the log files which exist under path <code >/var/log/containers/</code> on each host. The logs can be shipped to a target you configure below.
    clusterTarget: The cluster logging target is <code class="text-capitalize">{clusterTargetType}</code>. If project logging is enabled, logs will be shipped to both cluster target and project logging target.
    noClusterTarget: Logging is disabled at current cluster.
  tags:
    keyPlaceholder: e.g. foo
    valuePlaceholder: e.g. bar
    addActionLabel: Add Field
  keyValueForm:
    keyPlaceholder: e.g. 192.168.1.10
  targetKafka:
    addActionLabel: Add Broker
    host: Host
    port: Port
  logPreview:
    header: Log Format Preview
  additional:
    header: Additional Logging Configuration
    fields:
      header: Custom Log Fields
      helpText: You can add custom fields as key/value pairs for better filtering.
    flushInterval:
      header: Flush Interval
      label: Flush Interval
      placeholder: e.g. 1
      sec: sec
      helpText: How often buffered logs would be flushed.
    includeSystemComponent:
      label: Include System Log
  advanced:
    file: Edit as File
    cancel: Edit as a Form

  elasticsearch:
    header: Elasticsearch Configuration
    endpointHelpText: Copy your endpoint from Elastic Cloud, or input the reachable endpoint of your self-hosted Elacticsearch.
    endpointProtocolError: Endpoint should start with "http://" or "https://".
    endpointHostError: Please enter host name or domain name.
    xpack:
      header: X-Pack Security
      headerOptional: (optional)
      helpText: If your Elasticsearch use X-Pack built-in native authentication features, put your username and password below.
      username: Username
      usernamePlaceholder: e.g. John
      password: Password
      passwordPlaceholder: Your Password
    indexPatterns:
      header: Index Patterns
      helpText: Index patterns are used to generate Elacticsearch index
      prefix: Prefix
      prefixPlaceholder: e.g. logstash
      dateFormat: 'Date Format:'
      errors:
        startsWith: Prefix cannot start with -, _, +
        required: Prefix is required
        invalidCharacters: "Prefix cannot include '{char}'"

    generatedIndex: 'The generated index will be: <code class="text-italic text-lowercase">{esIndex}</code>, by default the index pattern is {indexFormat}'
    endpoint:
      required: '"Endpoint" is required'
  splunk:
    header: Splunk HTTP Event Collector Configuration
    helpText: 'You can find instructions on how to configre Splunk HEC(HTTP Event Collector) <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector" target="_blank">here</a>.'
    token: Token
    tokenPlaceholder: Your Token
    tokenHelpText: 'Tokens are entities that let logging agents and HTTP clients connect to the HEC input. <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Enterprise" target="_blank">Detail</a>'
    tokenRequired: Token is Required
    endpointRequired: Endpoint is Required
    source: Source
    sourcePlaceholder: e.g. fluentd
    sourceHelpText: 'A default field that identifies the source of an event, that is, where the event originated. <a href="https://docs.splunk.com/Splexicon:Source" target="_blank">Detail</a>'
    index: Index
    indexPlaceholder: e.g. main
    indexHelpText: 'The index you specify here must be within the list of this tokenâ€™s allowed indexes. <a href="https://docs.splunk.com/Splexicon:Index" target="_blank">Detail</a>'
    endpoint:
      placeholder: 'e.g. https://192.168.1.10:8088'
  kafka:
    header: Kafka Configuration
    endpointType: Endpoint Type
    zookeeper: Zookeeper
    broker: Broker
    brokerTypeHelpText: Use either Zookeeper or Broker list as the Kafka connection entrypoint.
    zookeeperHelpText: Zookeeper is for building coordination, configuration management, leader detection, and detecting node updates in a kafka cluster.
    brokerHelpText: A Kafka cluster consists of one or more Brokers, config the host and port for each Broker.
    addEndpoint: Add Endpoint
    topic:
      label: Topic
      required: '"Topic" is required'
    topicPlaceholder: e.g. message
    topicHelpText: Logs will be sent to this topic
    endpoint:
      required: '"Endpoint" is required'
      broker:
        placeholder: 'e.g. https://192.168.1.10:9092'
      zookeeper:
        placeholder: 'e.g. https://192.168.1.10:2181'
  syslog:
    endpointPlaceholder: 'e.g. 192.168.1.10:514'
    header: Syslog Configuration
    endpointHelpText: Input your Syslog endpoint, SSL certificates configuration will show up when select TCP.
    program: Program
    programPlaceholder: e.g. MyProgram
    programHelpText: The program name of the log.
    severityHelpText: '<p class="text-info text-small">The severity of logs. The definition of each can be found <a href="https://tools.ietf.org/html/rfc5424#section-6.2.1" target="_blank">here.</a></p>'
    severities:
      emergency: Emergency
      alert: Alert
      critical: Ctitical
      error: Error
      warning: Warning
      notice: Notice
      info: Info
      debug: Debug
    tokenHelpText: Will add a token to the structured data in every syslog message. For cloud syslog such as <a href="https://help.sumologic.com/Send-Data/Sources/02Sources-for-Hosted-Collectors/Cloud-Syslog-Source" target="_blank">Sumologic</a>, <a href="https://www.loggly.com/docs/customer-token-authentication-token" target="_blank">Loggly</a>, etc, you could generate a token on their respective configuration pages.
    endpoint:
      required: '"Endpoint" is required'
  fluentd:
    header: Fluentd Configuration
    compress:
      label: Enable Gzip Compression
      helpText: Enable this to reduce the transferred payload size.
    endpoint:
      label: Endpoint
      placeholder: 'e.g. 192.168.1.10:24224'
      invalid: '"Endpoint" required end with port.'
      required: '"Endpoint" is required'
    password:
      label: Password
      placeholder: The password for authentication.
    sharedKey:
      label: Shared Key
      placeholder: The Shared key for authentication.
    standby:
      label: Use as Standby Only
      none: Required at least one Non-Standby endpoint.
    username:
      label: Username
      placeholder: The username for authentication.
    weight:
      label: Weight
      placeholder: The load balancing weight of this endpoint.
    addServer: Add Fluentd Server
    endpointHostError: Endpoint can not start with 'http' or 'https', please use host name.
    endpointPortError: Endpoint required end with port.
    removeFluentServers: Remove this Fluentd Server
    enableTls:
      label: Use TLS
    certificate:
      label: CA Certificate PEM
    hostname:
      label: Hostname
      placeholder: The hostname of the server.
  cloudwatch:
    header: CloudWatch Configuration
    accessKeyID: Access Key ID
    accessKeyIDRequired: '"Access Key ID" is required'
    secretAccessKey: Secret Access Key
    secretAccessKeyRequired: '"Secret Access Key" is required'
    group: Log Group
    groupHelpText: This group must already exist
    groupRequired: '"Log Group" is required'
    stream: Log Stream
    streamHelpText: The stream will be created if it does not exist
    streamRequired: '"Log Stream" is required'
    region: Region
    regionPlaceholder: 'e.g. us-east-1'
    regionRequired: '"Region" is required'
  customTarget:
    type:
      error: "@type is not supported, you can chose the following type: ['elasticsearch', 'splunk_hec', 'remote_syslog', 'kafka_buffered', 'forward']"
  ssl:
    sslHeader: "{authType} Configuration"
    headerHelp: Input https endpoint or select TCP to enabled it.
    certificate:
      label: CA Certificate PEM
    clientKey:
      label: Client Private Key
    clientCert:
      label: Client Certificate
    clientKeyPass:
      label: Client Key Password
      password:
        placeholder: Your Client Key Password.
    verify:
      label: SSL Verify
      enabled: Enabled - Input trusted server certificate
      disabled: Disabled
    sslVersion:
      label: SSL Version
    saslType:
      label: Type
      plain: Plain
      scram: Scram
    saslUsername:
      label: Username
      placeholder: e.g. John
    saslPassword:
      label: Password
      placeholder: Your Password
    saslScramMechanism:
      label: Scram Mechanism
      sha256: sha256
      sha512: sha512
    customTarget:
      help: You can copy the path to the file.
      copyAll: Copy All key and value.

  dockerRootDir:
    header: Docker Configuration
    label: Docker Root Directory
    placeholder: "Enter docker root Directory, default value is {dir}"

  testAction:
    testing: TESTING
    testOk: Settings validated.
    testFailed: Something's not quite right. Check your inputs.
    test: TEST
    running: RUNNING
    runOk: Dry run success.
    dryRun: Dry Run
