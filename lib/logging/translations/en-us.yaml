loggingPage:
  targetNav:
    experimental: Experimental
    tips:
      caseA: Logging is disabled in the current {pageScope}.
      caseB: The current logging target is <code class="text-capitalize">{currentTarget}</code>, Click the Save button below will disable logging in current {pageScope}.
      caseC: The current logging target is <code class="text-capitalize">{currentTarget}</code>,
      caseD: click the Save button below to set <span class="text-info text-capitalize">{targetType}</span> as the logging target.
      caseE: click the Save button below to update the <span class="text-info text-capitalize">{targetType}</span> configuration.
      caseF: No logging target, click the Save button below to set <code class="text-capitalize">{targetType}</code> as the logging target.
  targetTypes:
    embedded: Embedded
    elasticsearch: Elasticsearch
    splunk: Splunk
    kafka: Kafka
    syslog: Syslog
    fluentd: Fluentd
    disable: None
  endpoint: Endpoint
  endpointPlaceholder: 'e.g. https://192.168.1.10:9200'
  logging: Logging
  clusterHeader: Cluster Logging
  projectHeader: Project Logging
  helpText:
    cluster: We will use fluentd to collect stdout/stderr logs from each container and the log files which under path <code >/var/log/containers/</code> on each host. The logs can be shipped to a target you configure below.
    clusterTarget: The cluster logging target is <code class="text-capitalize">{clusterTargetType}</code>. If project logging is enabled, logs will be shipped to both cluster target and project logging target.
    noClusterTarget: Logging is disabled at current cluster.
  tags:
    keyPlaceholder: e.g. foo
    valuePlaceholder: e.g. bar
    addActionLabel: Add Field
  keyValueForm:
    keyPlaceholder: e.g. 192.168.1.10
  targetKafka:
    addActionLabel: Add Broker
    host: Host
    port: Port
  logPreview:
    header: Log Format Preview
  additional:
    header: Additional Logging Configuration
    fields:
      header: Custom Log Fields
      helpText: You can add custom fields as key/value pairs for better filtering.
    flushInterval:
      header: Flush Interval
      label: Flush Interval
      placeholder: e.g. 1
      sec: sec
      helpText: How often buffered logs would be flushed.

  elasticsearch:
    header: Elasticsearch Configuration
    endpointHelpText: Copy your endpoint from Elastic Cloud, or input the reachable endpoint of your self-hosted Elacticsearch.
    endpointProtocolError: Endpoint should start with "http://" or "https://".
    endpointHostError: Please enter host name or domain name.
    xpack:
      header: X-Pack Security
      headerOptional: (optional)
      helpText: If your Elasticsearch use X-Pack built-in native authentication features, put your username and password below.
      username: Username
      usernamePlaceholder: e.g. John
      password: Password
      passwordPlaceholder: Your Password
    indexPatterns:
      header: Index Patterns
      helpText: Index patterns are used to generate Elacticsearch index
      prefix: Prefix
      prefixPlaceholder: e.g. logstash
      dateFormat: 'Date Format:'
      errors:
        startsWith: Prefix cannot start with -, _, +
        required: Prefix is required
        invalidCharacters: "Prefix cannot include '{char}'"
    generatedIndex: 'The generated index will be: <code class="text-italic text-lowercase">{esIndex}</code>, by default the index patters is {indexFormat}'
  splunk:
    header: Splunk HTTP Event Collector Configuration
    helpText: 'You can find instructions on how to configre Splunk HEC(HTTP Event Collector) <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector" target="_blank">here</a>.'
    token: Token
    tokenPlaceholder: Your Token
    tokenHelpText: 'Tokens are entities that let logging agents and HTTP clients connect to the HEC input. <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Enterprise" target="_blank">Detail</a>'
    source: Source
    sourcePlaceholder: e.g. fluentd
    sourceHelpText: 'A default field that identifies the source of an event, that is, where the event originated. <a href="https://docs.splunk.com/Splexicon:Source" target="_blank">Detail</a>'
    index: Index
    indexPlaceholder: e.g. main
    indexHelpText: 'The index you specify here must within the list of this tokenâ€™s allowed indexes. <a href="https://docs.splunk.com/Splexicon:Index" target="_blank">Detail</a>'
  kafka:
    header: Kafka Configuration
    endpointType: Endpoint Type
    zookeeper: Zookeeper
    broker: Broker
    brokerTypeHelpText: Use either Zookeeper or Broker list as the Kafka connection entrypoint.
    zookeeperHelpText: Zookeeper is for building coordination, configuration management, leader detection, detecting node update in kafka cluster.
    brokkerHelpText: A Kafka cluster consists of one or more Brokers, config the host and port for each Broker.
    addEndpoint: Add Endpoint
    topic: Topic
    topicPlaceholder: e.g. message
    topicHelpText: Logs will be send to this topic
  syslog:
    endpointPlaceholder: 'e.g. 192.168.1.10:514'
    header: Syslog Configuration
    endpointHelpText: Input your Syslog endpoint, SSL certificates configuration will show up when select TCP.
    program: Program
    programPlaceholder: e.g. MyProgram
    programHelpText: The program name of the log.
    severityHelpText: '<p class="text-info text-small">The severity of logs, list of severities definition could be found here <a href="https://tools.ietf.org/html/rfc5424#section-6.2.1" target="_blank">here.</a></p>'
    severities:
      emergency: Emergency
      alert: Alert
      critical: Ctitical
      error: Error
      warning: Warning
      notice: Notice
      info: Info
      debug: Debug
    tokenHelpText: Will add token to structured data in every syslog message. For cloud syslog like <a href="https://help.sumologic.com/Send-Data/Sources/02Sources-for-Hosted-Collectors/Cloud-Syslog-Source" target="_blank">Sumologic</a>, <a href="https://www.loggly.com/docs/customer-token-authentication-token" target="_blank">Loggly</a> etc, you could generate token on their configure page.
  fluentd:
    header: Fluentd Configuration
    compress:
      label: Enable Gzip Compression
      helpText: Enable this to reduce the transferred payload size.
    endpoint:
      label: Endpoint
      placeholder: 'e.g. 192.168.1.10:24224'
    password:
      label: Password
      placeholder: The password for authentication.
    sharedKey:
      label: Shared Key
      placeholder: The Shared key for authentication.
    standby:
      label: Use as Standby Only
    username:
      label: Username
      placeholder: The username for authentication.
    weight:
      label: Weight
      placeholder: The load balancing weight of this endpoint.
    addServer: Add Fluentd Server
    endpointHostError: Endpoint can not start with 'http' or 'https', please use host name.
    endpointPortError: Endpoint required end with port.
    removeFluentServers: Remove this Fluentd Server
    enableTls:
      label: Use TLS
    certificate:
      label: CA Certificate PEM
    hostname:
      label: Hostname
      placeholder: The hostname of the server.
  ssl:
    sslHeader: SSL Configuration
    certificate:
      label: CA Certificate PEM
    clientKey:
      label: Client Private Key
    clientCert:
      label: Client Certificate
    clientKeyPass:
      label: Client Key Password
      password:
        placeholder: Your Client Key Password.
    verify:
      label: SSL Verify
      enabled: Enabled - Input trusted server certificate
      disabled: Disabled
    sslVersion:
      label: SSL Version
  dockerRootDir:
    header: Docker Configuration
    label: Docker Root Directory
    placeholder: Enter docker root Directory, default value is {dir}
